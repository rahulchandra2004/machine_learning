{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ludXwQ_gM4Sz"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python-headless                #1\n",
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVqVxKOcM9dz"
      },
      "outputs": [],
      "source": [
        "import cv2                                                 #2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izw6z938NHsc"
      },
      "outputs": [],
      "source": [
        "model = load_model('/content/emotion_detection_model.h5')          #3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUgKwOwZNUkR"
      },
      "outputs": [],
      "source": [
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']  #4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Otp7nUQNXf1"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript     #5\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "        const div = document.createElement('div');\n",
        "        const capture = document.createElement('button');\n",
        "        capture.textContent = 'Capture';\n",
        "        document.body.appendChild(div);\n",
        "        div.appendChild(capture);\n",
        "\n",
        "        const video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "        div.appendChild(video);\n",
        "        video.srcObject = stream;\n",
        "        await new Promise((resolve) => video.onloadedmetadata = resolve);\n",
        "        video.play();\n",
        "\n",
        "        await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "        const canvas = document.createElement('canvas');\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        stream.getVideoTracks()[0].stop();\n",
        "        const dataUrl = canvas.toDataURL('image/jpeg', quality);\n",
        "        div.remove();\n",
        "        return dataUrl;\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "try:\n",
        "    filename = take_photo()\n",
        "    print('Saved to {}'.format(filename))\n",
        "\n",
        "    # Show the image\n",
        "    img = cv2.imread(filename)\n",
        "    cv2_imshow(img)  # Use cv2_imshow here\n",
        "\n",
        "except Exception as err:\n",
        "    print(str(err))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2P5xsGdNmcX"
      },
      "outputs": [],
      "source": [
        "# Read the captured image                                   #6\n",
        "frame = cv2.imread('photo.jpg')\n",
        "\n",
        "# Convert to grayscale\n",
        "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Load OpenCV's pre-trained face detector\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Detect faces\n",
        "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "for (x, y, w, h) in faces:\n",
        "    # Extract the face ROI\n",
        "    roi_gray = gray[y:y + h, x:x + w]\n",
        "    roi_gray = cv2.resize(roi_gray, (48, 48))\n",
        "    roi = roi_gray.astype(\"float\") / 255.0\n",
        "    roi = img_to_array(roi)\n",
        "    roi = np.expand_dims(roi, axis=0)\n",
        "\n",
        "    # Predict the emotion\n",
        "    preds = model.predict(roi)[0]\n",
        "    emotion = emotion_labels[preds.argmax()]\n",
        "\n",
        "    # Draw the face bounding box and label\n",
        "    cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "    cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "# Display the resulting frame\n",
        "cv2_imshow(frame)  # Use cv2_imshow here\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}